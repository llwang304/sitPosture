# import os
#
# from dotenv import load_dotenv, find_dotenv
# _ = load_dotenv(find_dotenv()) # read local .env file
#
# # account for deprecation of LLM model
# import datetime
# # Get the current date
# current_date = datetime.datetime.now().date()
#
# # Define the date after which the model should be set to "gpt-3.5-turbo"
# target_date = datetime.date(2024, 6, 12)
#
# # Set the model variable based on the current date
# if current_date > target_date:
#     llm_model = "gpt-3.5-turbo"
# else:
#     llm_model = "gpt-3.5-turbo-0301"
#
#
#
import os
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from buildvectorstore import build_vectorstore
from mongoConversationLoader import MongoConversationLoader
from langchain.chains import ConversationChain

# llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", google_api_key=os.getenv("GOOGLE_API_KEY"))
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", google_api_key='AIzaSyD_YZRrOIZ11clBspqGhZyRkwvvrikRX-A')

phone = "13800138000"
conversation_id = 1

# ä» MongoDB åŠ è½½å†å²è®°å¿†
mongo_loader = MongoConversationLoader(
    mongo_uri="mongodb://localhost:27017/",
    db_name="sitPostureApp",
    collection_name="conversations",
    llm=llm
)
#æ–¹æ³•1 é€‚ç”¨äºé•¿å¯¹è¯ï¼Œè®°å¿†ç»„æˆï¼šlangchain.summarymemory
# memory = mongo_loader.load_conversation_memory(phone, conversation_id)
#æ–¹æ³•2 é€‚ç”¨äºçŸ­å¯¹è¯ï¼Œè®°å¿†ç»„æˆï¼šå†…å­˜
memory = ConversationBufferMemory(memory_key="history", return_messages=True)
#æ–¹æ³•3 é€‚ç”¨äºé•¿å¯¹è¯ï¼Œè®°å¿†ç»„æˆï¼šå†å²è®°å½•ç»æ‰‹åŠ¨æ‹¼æ¥ç”±å¤§æ¨¡å‹ç”Ÿæˆæ€»ç»“
# å°†æ€»ç»“å†…å®¹ä½œä¸ºå†å²å¯¹è¯çš„ä¸€éƒ¨åˆ†åŠ å…¥åˆ° memory
# summary=mongo_loader.get_conversation_summary(phone=phone, conversation_id=conversation_id)
# memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
# # ä¿å­˜åŠ©æ‰‹çš„æ€»ç»“åˆ° memory
# memory.save_context(inputs={"user": ""}, outputs={"assistant": summary})  # è¿™é‡Œä¿å­˜åŠ©æ‰‹çš„æ¶ˆæ¯




# ä½ å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–¹æ³•æ¥å¤„ç†ä» retriever å’Œ llm è·å–ç­”æ¡ˆ

    # å°è¯•ä» retriever è·å–ç­”æ¡ˆ
    # æµ‹è¯•å¯¹è¯ï¼ˆä½ å¯ä»¥æ”¹æˆ flask æ¥å£æˆ– UI é¡µé¢ç­‰ï¼‰
user_input = "å·¥ä½œç›¸å…³è‚Œè‚‰éª¨éª¼ç–¾æ‚£çš„é¢„é˜²ï¼Ÿ"
# æ„å»ºå¯¹è¯é“¾ï¼ˆæ— RAGï¼‰
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)
response = conversation.predict(input=user_input)
print(response)

# ğŸš€ å¼€å§‹æé—®ï¼š
# response = qa_chain.invoke({"question": "æˆ‘ä¹‹å‰é—®è¿‡ä½ ä»€ä¹ˆé—®é¢˜ï¼Ÿ"})
# print(response["answer"])


# è°ƒç”¨è¿™ä¸ªæ–¹æ³•è·å–å›ç­”
# response = get_answer("å·¥ä½œç›¸å…³è‚Œè‚‰éª¨éª¼ç–¾æ‚£çš„é¢„é˜²ï¼Ÿ")
# print(response)
#
