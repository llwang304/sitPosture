# import os
#
# from dotenv import load_dotenv, find_dotenv
# _ = load_dotenv(find_dotenv()) # read local .env file
#
# # account for deprecation of LLM model
# import datetime
# # Get the current date
# current_date = datetime.datetime.now().date()
#
# # Define the date after which the model should be set to "gpt-3.5-turbo"
# target_date = datetime.date(2024, 6, 12)
#
# # Set the model variable based on the current date
# if current_date > target_date:
#     llm_model = "gpt-3.5-turbo"
# else:
#     llm_model = "gpt-3.5-turbo-0301"
#
#
#
import os
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from buildvectorstore import build_vectorstore
from mongoConversationLoader import MongoConversationLoader

#åˆå§‹åŒ–
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2",model_kwargs={'device': 'cpu'})
doc_path="doc_new"
vectorstore_path="vectorstore"

# å¦‚æœå·²å­˜åœ¨ vectorstoreï¼Œå°±ç›´æ¥åŠ è½½
if os.path.exists(vectorstore_path):
    print("ğŸŸ¢ å·²æ£€æµ‹åˆ°å‘é‡åº“ï¼Œæ­£åœ¨åŠ è½½...")
    vectorstore = FAISS.load_local(vectorstore_path, embedding_model,allow_dangerous_deserialization=True)
else:
    print("ğŸŸ¡ æœªæ£€æµ‹åˆ°å‘é‡åº“ï¼Œå¼€å§‹å‘é‡åŒ–...")
    build_vectorstore(doc_path,vectorstore_path)
    vectorstore = FAISS.load_local(vectorstore_path, embedding_model, allow_dangerous_deserialization=True)


# llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", google_api_key=os.getenv("GOOGLE_API_KEY"))
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", google_api_key='AIzaSyD_YZRrOIZ11clBspqGhZyRkwvvrikRX-A')
# todo:retrieverå‚æ•°è§gpt
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})# k è¿”å›ç›¸ä¼¼æ–‡æ¡£çš„æ•°é‡ï¼ˆé»˜è®¤ 4ï¼‰, score_threshold 	åªè¿”å›ç›¸ä¼¼åº¦é«˜äºè¯¥é˜ˆå€¼çš„æ–‡æ¡£ï¼ˆæ¯”å¦‚ 0.7ï¼‰

phone = "13800138000"
conversation_id = 1

# ä» MongoDB åŠ è½½å†å²è®°å¿†
mongo_loader = MongoConversationLoader(
    mongo_uri="mongodb://localhost:27017/",
    db_name="sitPostureApp",
    collection_name="conversations",
    llm=llm
)
#æ–¹æ³•1 é€‚ç”¨äºé•¿å¯¹è¯ï¼Œè®°å¿†ç»„æˆï¼šlangchain.summarymemory
# memory = mongo_loader.load_conversation_memory(phone, conversation_id)
#æ–¹æ³•2 é€‚ç”¨äºçŸ­å¯¹è¯ï¼Œè®°å¿†ç»„æˆï¼šå†…å­˜
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
#æ–¹æ³•3 é€‚ç”¨äºé•¿å¯¹è¯ï¼Œè®°å¿†ç»„æˆï¼šå†å²è®°å½•ç»æ‰‹åŠ¨æ‹¼æ¥ç”±å¤§æ¨¡å‹ç”Ÿæˆæ€»ç»“
# å°†æ€»ç»“å†…å®¹ä½œä¸ºå†å²å¯¹è¯çš„ä¸€éƒ¨åˆ†åŠ å…¥åˆ° memory
# summary=mongo_loader.get_conversation_summary(phone=phone, conversation_id=conversation_id)
# memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
# # ä¿å­˜åŠ©æ‰‹çš„æ€»ç»“åˆ° memory
# memory.save_context(inputs={"user": ""}, outputs={"assistant": summary})  # è¿™é‡Œä¿å­˜åŠ©æ‰‹çš„æ¶ˆæ¯




# ä½ å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–¹æ³•æ¥å¤„ç†ä» retriever å’Œ llm è·å–ç­”æ¡ˆ
def get_answer(question: str):
    # å°è¯•ä» retriever è·å–ç­”æ¡ˆ
    retrieved_docs = retriever.invoke("æ‰‹æ‰˜æ€ä¹ˆå¸ƒç½®ï¼Ÿ")
    if retrieved_docs:
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            memory=memory,
            verbose=True,
            return_source_documents = True
        )
        return qa_chain.invoke(question)
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œç›´æ¥é€šè¿‡ LLM ç”Ÿæˆç­”æ¡ˆ
        return llm.call(question)
# ğŸš€ å¼€å§‹æé—®ï¼š
# response = qa_chain.invoke({"question": "æˆ‘ä¹‹å‰é—®è¿‡ä½ ä»€ä¹ˆé—®é¢˜ï¼Ÿ"})
# print(response["answer"])


# è°ƒç”¨è¿™ä¸ªæ–¹æ³•è·å–å›ç­”
response = get_answer("æ‰‹æ‰˜æ€ä¹ˆå¸ƒç½®ï¼Ÿ")
print(response)
