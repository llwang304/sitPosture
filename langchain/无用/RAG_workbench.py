import os
from langchain_community.chat_models import ChatTongyi # ç”¨é˜¿é‡ŒOwen
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from buildvectorstore import build_vectorstore
#from mongoConversationLoader import MongoConversationLoader
from dotenv import load_dotenv
#1.åŠ è½½.env æ–‡ä»¶ä¸­çš„APIå¯†é’¥
load_dotenv()
dashscope_api_key = os.getenv("DASHSCOPE_API_KEY") # é€šä¹‰åƒé—®API Key
def load_vectorstore(doc_path: str, vectorstore_path: str, build_vectorstore_fn):
    #åˆå§‹åŒ–
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2",
                                            model_kwargs={'device': 'cpu'})
    # å¦‚æœå·²å­˜åœ¨ vectorstoreï¼Œå°±ç›´æ¥åŠ è½½
    if os.path.exists(vectorstore_path):
        print("ğŸŸ¢ å·²æ£€æµ‹åˆ°å‘é‡åº“ï¼Œæ­£åœ¨åŠ è½½...")
        vectorstore = FAISS.load_local(
            vectorstore_path,
            embedding_model,
            allow_dangerous_deserialization=True
        )
    else:
        print("ğŸŸ¡ æœªæ£€æµ‹åˆ°å‘é‡åº“ï¼Œå¼€å§‹å‘é‡åŒ–...")
        build_vectorstore(doc_path,vectorstore_path)
        vectorstore = FAISS.load_local(
            vectorstore_path,
            embedding_model,
            allow_dangerous_deserialization=True
        )
    return vectorstore



# phone = "13800138000"
# conversation_id = 1
#
# # ä» MongoDB åŠ è½½å†å²è®°å¿†
# mongo_loader = MongoConversationLoader(
#     mongo_uri="mongodb://localhost:27017/",
#     db_name="sitPostureApp",
#     collection_name="conversations",
#     llm=llm
# )
def query_workbench_doc(query:str):
    # llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", google_api_key=os.getenv("GOOGLE_API_KEY"))
    # llm = ChatGoogleGenerativeAI(
    #     model="gemini-2.0-flash",
    #     google_api_key='AIzaSyD_YZRrOIZ11clBspqGhZyRkwvvrikRX-A')
    llm = ChatTongyi(model_name="qwen-turbo", temperature=0, dashscope_api_key=dashscope_api_key)
    # ä» MongoDB åŠ è½½å†å²è®°å¿†
    # mongo_loader = MongoConversationLoader(
    #     mongo_uri="mongodb://localhost:27017/",
    #     db_name="sitPostureApp",
    #     collection_name="conversations",
    #     llm=llm
    # )
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
    output_key = "answer"
    )
    doc_path = "doc_new"
    vectorstore_path = "vectorstore"
    vectorstore=load_vectorstore(doc_path, vectorstore_path, build_vectorstore)
    # todo:retrieverå‚æ•°è§gpt
    retriever = vectorstore.as_retriever(
        search_kwargs={"k": 3})  #score_threshold 	åªè¿”å›ç›¸ä¼¼åº¦é«˜äºè¯¥é˜ˆå€¼çš„æ–‡æ¡£ï¼ˆæ¯”å¦‚ 0.7ï¼‰
    # å¯åŠ¨ RAG
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        verbose=True,
        return_source_documents=True,
    # output_key = "answer"  # ğŸ‘ˆ åŠ ä¸Šè¿™ä¸ªå°±èƒ½è§£å†³æŠ¥é”™
    )
    response = qa_chain.invoke(query)
    return response["answer"] if isinstance(response, dict) and "answer" in response else response

# from langchain.llms import HuggingFacePipeline
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# from langchain.chains import RetrievalQA
# from langchain.prompts import PromptTemplate
# prompt_template = """
# ä½ æ˜¯ä¸€ä¸ªäººä½“å·¥æ•ˆå­¦ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹èµ„æ–™å†…å®¹å›ç­”ç”¨æˆ·å…³äºå·¥ä½œå°å¸ƒç½®çš„é—®é¢˜ã€‚å¦‚æœæ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·è¯šå®åœ°è¯´ä½ ä¸çŸ¥é“ã€‚
#
# ----------------
# {context}
# ----------------
# ç”¨æˆ·é—®é¢˜ï¼š{question}
# ä¸“å®¶å›ç­”ï¼š
# """
# model_id = "Qwen/Qwen1.5-4B-Chat"
#
# tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
# model = AutoModelForCausalLM.from_pretrained(
#     model_id,
#     # device_map="auto",
#     trust_remote_code=True,
#     torch_dtype="auto"
# )
#
# qwen_pipeline = pipeline(
#     "text-generation",
#     model=model,
#     tokenizer=tokenizer,
#     max_new_tokens=512,
#     do_sample=True,
#     temperature=0.7,
#     top_p=0.95,
# )
#
# llm = HuggingFacePipeline(pipeline=qwen_pipeline)
# prompt = PromptTemplate(
#     input_variables=["context", "question"],
#     template=prompt_template
# )
#
#
#
#
# def query_from_doc(query:str):
#     # æ¨èæ¨¡å‹
#     doc_path = "doc_new"
#     vectorstore_path = "vectorstore"
#     vectorstore = load_vectorstore(doc_path, vectorstore_path, build_vectorstore)
#     retriever = vectorstore.as_retriever(
#         search_kwargs={"k": 3})  # score_threshold 	åªè¿”å›ç›¸ä¼¼åº¦é«˜äºè¯¥é˜ˆå€¼çš„æ–‡æ¡£ï¼ˆæ¯”å¦‚ 0.7ï¼‰
#     rag_chain = RetrievalQA.from_chain_type(
#         llm=llm,
#         chain_type="stuff",
#         retriever=retriever,
#         return_source_documents=True,
#         chain_type_kwargs={"prompt": prompt}
#     )
#     result = rag_chain({"query": query})
#     return result
#

import requests
# 1. Qwen API è°ƒç”¨å‡½æ•°ï¼ˆæ›¿æ¢æˆä½ è‡ªå·±çš„API KEYå’Œåœ°å€ï¼‰
# def qwen_api_generate(prompt, max_tokens=512):
#     api_url = "https://api.qwen.com/v1/chat/completions"  # ä¸¾ä¾‹æ¥å£
#     headers = {
#         "Authorization": dashscope_api_key,
#         "Content-Type": "application/json"
#     }
#     data = {
#         "model": "qwen-turbo",  # æˆ– qwen-1.5bï¼ŒæŒ‰ä½ APIæ”¯æŒçš„å‹å·å¡«
#         "messages": [{"role": "user", "content": prompt}],
#         "max_tokens": max_tokens,
#         "temperature": 0.7,
#     }
#     response = requests.post(api_url, json=data, headers=headers)
#     resp_json = response.json()
#     return resp_json["choices"][0]["message"]["content"]

# Qwen API è°ƒç”¨å‡½æ•°ç¤ºèŒƒï¼ˆæ›¿æ¢ä¸ºä½ çš„APIåœ°å€å’Œå‚æ•°ï¼‰
def query_qwen_api(prompt: str) -> str:
    url = "https://api.qwen.example/v1/chat/completions"  # æ›¿æ¢æˆå®é™…APIåœ°å€
    headers = {
        "Authorization": "Bearer your_api_key",
        "Content-Type": "application/json"
    }
    data = {
        "model": "qwen-turbo",  # å…·ä½“æ¨¡å‹åæ ¹æ®API
        "messages": [
            {"role": "user", "content": prompt}
        ]
    }
    response = requests.post(url, headers=headers, json=data)
    response.raise_for_status()
    res_json = response.json()
    # æŒ‰Qwen APIè¿”å›æ ¼å¼è§£æå›ç­”ï¼Œç¤ºèŒƒç”¨res_json["choices"][0]["message"]["content"]
    return res_json["choices"][0]["message"]["content"]


# 2. ç®€æ˜“æ–‡æœ¬æ£€ç´¢å‡½æ•°ï¼ˆæ¨¡æ‹Ÿï¼‰
def retrieve_relevant_docs(query, docs, top_k=3):
    # è¿™é‡Œç®€å•ç”¨åŒ…å«å…³é”®è¯çš„æ–‡æœ¬è¿‡æ»¤ä»£æ›¿å‘é‡æ£€ç´¢
    relevant = [doc for doc in docs if query.lower() in doc.lower()]
    return relevant[:top_k]

# è‡ªå®šä¹‰ä¸€ä¸ªç®€å•çš„ Chain æ¥ç»“åˆæ£€ç´¢å’ŒQwen API
from langchain.chains.base import Chain
from typing import Dict
from langchain.schema import BaseRetriever
from pydantic import Field

class QwenRAGChain(Chain):
    retriever: BaseRetriever = Field()


    @property
    def input_keys(self):
        return ["question"]

    @property
    def output_keys(self):
        return ["answer"]

    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:
        question = inputs["question"]
        # ç”¨retrieveræ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
        docs = self.retriever.get_relevant_documents(question)
        context = "\n".join([doc.page_content for doc in docs])
        # æ‹¼æ¥æ£€ç´¢ç»“æœå’Œé—®é¢˜åšæç¤º
        prompt = f"æ ¹æ®ä»¥ä¸‹å†…å®¹å›ç­”é—®é¢˜ï¼š\n{context}\né—®é¢˜ï¼š{question}\nå›ç­”ï¼š"
        # è°ƒç”¨è¿œç¨‹Qwen APIç”Ÿæˆç­”æ¡ˆ
        answer = query_qwen_api(prompt)
        return {"answer": answer}


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    doc_path = "doc_new"  # ä½ æ–‡æ¡£è·¯å¾„
    vectorstore_path = "vectorstore"

    # åŠ è½½æˆ–æ„å»ºå‘é‡åº“
    vectorstore = load_vectorstore(doc_path, vectorstore_path, build_vectorstore)

    # åˆ›å»ºRAG chain
    rag_chain = QwenRAGChain(retriever=vectorstore.as_retriever())

    # æµ‹è¯•é—®ç­”
    question = "åœ¨å·¥ä½œå°å¸ƒç½®ä¸­ï¼Œæ¤…å­æœ‰ä»€ä¹ˆæ³¨æ„äº‹é¡¹ï¼Ÿ"
    result = rag_chain.run(question)
    print("é—®ï¼š", question)
    print("ç­”ï¼š", result)